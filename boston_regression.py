# -*- coding: utf-8 -*-
"""boston_regression_correct_one

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1evW4I-GAJxBSGQ9B6GjvslyuFG479ya7
"""

import tensorflow as tf
import numpy as np
from tensorflow.contrib import learn


from sklearn.pipeline import Pipeline
from sklearn import datasets, linear_model
from sklearn import cross_validation
from matplotlib import pyplot as plt

boston = learn.datasets.load_dataset('boston')
x, y = boston.data, boston.target
X_train_un, X_test_un, Y_train_vec, Y_test_vec = cross_validation.train_test_split(
x, y, test_size=0.2, random_state=42)

X_train=X_train_un/X_train_un.max(0)
X_test=X_test_un/X_test_un.max(0)


Y_train=np.reshape(Y_train_vec,(Y_train_vec.shape[0],1))
#print(Y_train)
Y_test=np.reshape(Y_test_vec,(Y_test_vec.shape[0],1))

# X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(
# x, y, test_size=0.2, random_state=42)

total_len = X_train.shape[0]

arr_idx=np.arange(total_len)


# Parameters
learning_rate = 0.0001
eps_scale=0.001
training_epochs = 10
#batch_size = 10
display_step = 10
batch_size =64


sample_num=5

g_thresh=1

n_input=13
n_output=1
n_hidden=20

training_iters=1000000
display_step=10

print(X_train.max(0))


print(X_train.shape)

#print(g)

    # First create placeholders for inputs and targets: x_input, y_target
x_input = tf.placeholder(tf.float32, shape=[None, n_input])
y_target = tf.placeholder(tf.float32, shape=[None, n_output])


parameters = tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden, 1]), tf.truncated_normal([n_hidden, 1]),
                            tf.truncated_normal([n_hidden * n_output,1]), tf.truncated_normal([n_output, 1])],0))
para_cpy=tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden, 1]), tf.truncated_normal([n_hidden, 1]),
                        tf.truncated_normal([n_hidden * n_output,1]), tf.truncated_normal([n_output, 1])],0))
save_para=para_cpy.assign(parameters)
load_para=parameters.assign(save_para)
para_size=parameters.get_shape().as_list()

rand_dir = tf.placeholder(tf.float32,shape=para_size)
with tf.name_scope("hidden") as scope:
    idx_from = 0 
    hidden_weights = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[n_input*n_hidden, 1]), [n_input, n_hidden])
    idx_from = idx_from + n_input*n_hidden
    hidden_biases = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[n_hidden, 1]), [n_hidden]) # tf.Variable(tf.truncated_normal([n_hidden]))
    hidden =  tf.nn.relu(tf.matmul(x_input, hidden_weights) + hidden_biases)
with tf.name_scope("linear") as scope:
    idx_from = idx_from + n_hidden
    linear_weights = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[n_hidden, 1]), [n_hidden,1])
    idx_from = idx_from + n_hidden*n_output
    linear_biases = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[n_output, 1]), [n_output]) 
    output = tf.matmul(hidden, linear_weights) + linear_biases
#     linear_weights = tf.slice(parameters, begin=[idx_from, 0], size=[n_hidden*n_output, 1])
#     idx_from = idx_from + n_hidden*n_output
#     linear_biases = tf.slice(parameters, begin=[idx_from, 0], size=[n_output, 1])
#     output = tf.matmul(hidden, linear_weights) + linear_biases
    
    
# Define cross entropy loss
loss = tf.reduce_mean(tf.square(output-y_target))
#pred= output
tvars = tf.trainable_variables()[0]
dloss_dw = tf.gradients(loss, parameters)[0]
df_dw= tf.gradients(output,parameters)[0]
d2f_dw2=tf.squeeze(tf.hessians(output, parameters))
grad_norm=tf.norm(dloss_dw)
dim, _ = dloss_dw.get_shape().as_list()
new_parameters=parameters.assign(parameters-learning_rate*dloss_dw)

rand_parameters=parameters.assign(parameters-eps_scale*rand_dir)
hess=tf.squeeze(tf.hessians(loss, parameters))
init_op = tf.initialize_all_variables()
#     print(dim)
#     hess =tf.Variable(tf.random_normal([dim,dim]))
#     for i in range(dim):
#         print(i)
#         dfx_i = tf.slice(dloss_dw, begin=[i,0] , size=[1,1])
#         ddfx_i = tf.gradients(dfx_i, parameters)[0] # whenever we use tf.gradients, make sure you get the actual tensors by putting [0] at the end
#         print(ddfx_i)
#         #gd_vec=list(ddfx_i)
#         update_hess=hess[:,i].assign(ddfx_i)
#         #.append(ddfx_i)  
#         #print(hess)
#     hess = tf.squeeze(hess) 

#

save_min_egvl=np.zeros([training_iters,1]) ##minimum eigen_value## Please visualize
#save_min_egvec=np.zeros([training_iters,1]) ##eigen_vector##
save_spec=np.zeros([training_iters,sample_num]) #spectrum_value ## Please visualize
save_proj_grad=np.zeros([training_iters,sample_num]) # projected gradient norm ## Please visualize
save_grad_norm=np.zeros([training_iters,1]) #gradient norm ## Please visualize
#save_min_diff_egvl=np.zeros([training_iters,1]) 



with tf.Session() as sess:
    sess.run(init_op)
    writer=tf.summary.FileWriter("./",sess.graph)
    step=0
    while step < training_iters:
        np.random.shuffle(arr_idx)
        batch_xs= X_train[arr_idx[0:batch_size],:]
        batch_ys= Y_train[arr_idx[0:batch_size]]
#         print(batch_size)
#         print(arr_idx[0:batch_size])
        #print(batch_xs)
        #print(batch_ys)
        #print(batch_xs.shape)
        feed_dict = {x_input: batch_xs, y_target: batch_ys }
        #print(sess.run(loss, feed_dict))
        para,_=sess.run([new_parameters,loss],feed_dict)
        fit_val=sess.run(output,feed_dict)
        sess.run(save_para,feed_dict)
        hessian=sess.run(hess,feed_dict)       
        grad=sess.run(dloss_dw,feed_dict)
        grad_norm_val=sess.run(grad_norm,feed_dict)        
        
        #######################################
        ################################
#         d2f=sess.run(d2f_dw2,feed_dict)
#         df=sess.run(df_dw,feed_dict)
#         hand_hess=np.dot(df,np.transpose(df))+(fit_val-batch_ys)*d2f
#         mat_diff=hessian-2*hand_hess
        ##################################        
        #######################################
        pred=sess.run(output,feed_dict={x_input: X_test, y_target:Y_test})
        c=sess.run(loss,feed_dict={x_input: X_test, y_target:Y_test})       
        s, U = np.linalg.eig(hessian)        
        save_min_egvl[step]=min(s)        
        save_grad_norm[step]=grad_norm_val
        fix_hess=np.copy(np.abs(hessian)>0.0001)
#         if (min(s)<0 and grad_norm_val<g_thresh):
        for i in range(sample_num):
            np.random.shuffle(arr_idx)
            tmp_batch_xs= X_train[arr_idx[0:batch_size],:]
            tmp_batch_ys= Y_train[arr_idx[0:batch_size]]
            tmp_feed_dict = {x_input: tmp_batch_xs, y_target: tmp_batch_ys }
            tmp_grad=sess.run(dloss_dw,tmp_feed_dict)            
            save_proj_grad[step,i]=np.abs(np.inner(U[:,dim-1],tmp_grad[:,0]))                   
        for i in range(sample_num):
            rand_vec=np.reshape(np.random.normal(0,1,np.prod(para_size)),para_size)
            norm_val=np.linalg.norm(rand_vec)
            np_rand_dir=rand_vec/norm_val
            sess.run(rand_parameters,feed_dict={rand_dir:  np_rand_dir})
            hessian=sess.run(hess,feed_dict)
            diff_hess=fix_hess-hessian
            s, U = np.linalg.eig(diff_hess)
            save_spec[step,i]=max(np.abs(s))
            sess.run(load_para,feed_dict)
        
        step+=1
        #print(step)
        if (step%display_step) == 0:
              print('step is :', step)
              print('min_grad_norm:',min(save_grad_norm[0:step]))
              print('min_eigen_val:',min(save_min_egvl[0:step]))
              print('current_loss:',c)
              np.savez_compressed('./',spec=save_spec[0:(step+1)],grad_proj=save_proj_grad[0:(step+1)],grad_val=save_grad_norm[0:(step+1)]
                ,min_eigen=save_min_egvl[0:(step+1)])